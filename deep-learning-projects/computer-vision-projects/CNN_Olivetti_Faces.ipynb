{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asgardian1196/asg-ml/blob/main/CNN_Olivetti_Faces.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetching Olivetti Dataset"
      ],
      "metadata": {
        "id": "7L81SGbe8ceA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "faces = datasets.fetch_olivetti_faces()\n",
        "faces.data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIKcVpwVslds",
        "outputId": "a1102114-2548-4faa-dafa-f49f118be9eb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading Olivetti faces from https://ndownloader.figshare.com/files/5976027 to /root/scikit_learn_data\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400, 4096)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating test-train split"
      ],
      "metadata": {
        "id": "PoYVwaxk3gWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(faces.data, faces.target, random_state=0)\n",
        "\n",
        "print(X_train.shape, X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrSpg0gDs5DN",
        "outputId": "894a01ea-ce30-413a-e39a-41cd6661e29d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300, 4096) (100, 4096)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reshaping data for CNN "
      ],
      "metadata": {
        "id": "7Y6HPWlP3tBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape(-1,64,64,1)\n",
        "X_test = X_test.reshape(-1,64,64,1)\n",
        "\n",
        "print(\"X_train: \",X_train.shape)\n",
        "print(\"X_test: \",X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNTAzcui14iW",
        "outputId": "555c7710-a59f-49e1-dbf1-a7f804bec84a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train:  (300, 64, 64, 1)\n",
            "X_test:  (100, 64, 64, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-Hot encoding for target data"
      ],
      "metadata": {
        "id": "ULLUF_8r32cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.np_utils import to_categorical \n",
        "\n",
        "y_train_ = to_categorical(y_train, num_classes = 40)\n",
        "y_test_ = to_categorical(y_test, num_classes = 40)\n",
        "\n",
        "print(\"y_train_ shape: \",y_train_.shape)\n",
        "print(\"y_test_ shape: \",y_test_.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdYEFvLq14jN",
        "outputId": "baa46a0b-4e42-4b9e-b07d-04448806b81c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y_train_ shape:  (300, 40)\n",
            "y_test_ shape:  (100, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "cA21XYOI4KmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau\n"
      ],
      "metadata": {
        "id": "1oALdvE-4IlL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the sequential model"
      ],
      "metadata": {
        "id": "RuJCeJ8d4Omm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n"
      ],
      "metadata": {
        "id": "Bkt2jnL44N-8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding Layers"
      ],
      "metadata": {
        "id": "XGKzqzkU4TIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a 2D convolution layer with 20 filters\n",
        "\n",
        "model.add(Conv2D(filters = 20, \n",
        "                 kernel_size = (5,5),\n",
        "                 padding = 'Same', \n",
        "                 activation ='relu',\n",
        "                 input_shape = (64,64,1)))"
      ],
      "metadata": {
        "id": "ePGB1YK44qp-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Max pooling operation for 2D spatial data.\n",
        "model.add(MaxPool2D(pool_size=(2,2)))"
      ],
      "metadata": {
        "id": "FH_GYgY045X8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Dropout to the input.\n",
        "model.add(Dropout(0.25))"
      ],
      "metadata": {
        "id": "_63fG_TK5HHK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding another 2D convolution layer, with 50 filters\n",
        "model.add(Conv2D(filters = 50,\n",
        "                 kernel_size = (6,6),\n",
        "                 padding = 'Same', \n",
        "                 activation ='relu'))"
      ],
      "metadata": {
        "id": "vSqDKShH5RSE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MaxPool and dropout\n",
        "model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25))"
      ],
      "metadata": {
        "id": "OLL7a3HT5pNM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding next 2D convolution layer with 150 filters\n",
        "model.add(Conv2D(filters = 150,\n",
        "                 kernel_size = (5,5),\n",
        "                 padding = 'Same', \n",
        "                 activation ='relu', \n",
        "                 input_shape = (64,64,1)))"
      ],
      "metadata": {
        "id": "3eECD6ia5yXv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the layer\n",
        "# Keras.layers.flatten function flattens the multi-dimensional input tensors into a single dimension\n",
        "\n",
        "model.add(Flatten())\n"
      ],
      "metadata": {
        "id": "PdnmX72_6Q_N"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense Layer \n",
        "model.add(Dense(256, activation = \"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(40, activation = \"softmax\"))"
      ],
      "metadata": {
        "id": "hOmzCviC6eHi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizer"
      ],
      "metadata": {
        "id": "oLGZb1ns6vdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = RMSprop(learning_rate=0.001, \n",
        "                    rho=0.9, \n",
        "                    epsilon=1e-08,\n",
        "                    decay=0.0)"
      ],
      "metadata": {
        "id": "mvaCrG2q6ec8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n"
      ],
      "metadata": {
        "id": "IrmuUBcW6ed1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image Data Generator"
      ],
      "metadata": {
        "id": "wdG2H4za7LtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=5,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.05, # Randomly zoom image \n",
        "        width_shift_range=0,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=False,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images"
      ],
      "metadata": {
        "id": "-dyKTgdJ7Ke2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datagen.fit(X_train)"
      ],
      "metadata": {
        "id": "hDrQVt8y7Wpn"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 20\n",
        "epoch = 37\n",
        "\n",
        "history = model.fit(datagen.flow(X_train,y_train_, batch_size=batch_size),\n",
        "                    epochs = epoch, \n",
        "                    validation_data = (X_test,y_test_),\n",
        "                    verbose = 2, \n",
        "                    steps_per_epoch=X_train.shape[0] // batch_size                             )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlFdXNH-7ayo",
        "outputId": "156af786-af91-4a91-f5da-ee84bb209f3a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/37\n",
            "15/15 - 11s - loss: 4.4905 - accuracy: 0.0067 - val_loss: 3.6917 - val_accuracy: 0.0100 - 11s/epoch - 744ms/step\n",
            "Epoch 2/37\n",
            "15/15 - 0s - loss: 3.6913 - accuracy: 0.0267 - val_loss: 3.6937 - val_accuracy: 0.0100 - 250ms/epoch - 17ms/step\n",
            "Epoch 3/37\n",
            "15/15 - 0s - loss: 3.6904 - accuracy: 0.0300 - val_loss: 3.7016 - val_accuracy: 0.0100 - 238ms/epoch - 16ms/step\n",
            "Epoch 4/37\n",
            "15/15 - 0s - loss: 3.7189 - accuracy: 0.0433 - val_loss: 3.6968 - val_accuracy: 0.0000e+00 - 230ms/epoch - 15ms/step\n",
            "Epoch 5/37\n",
            "15/15 - 0s - loss: 3.7006 - accuracy: 0.0200 - val_loss: 3.6977 - val_accuracy: 0.0100 - 244ms/epoch - 16ms/step\n",
            "Epoch 6/37\n",
            "15/15 - 0s - loss: 3.6733 - accuracy: 0.0467 - val_loss: 3.6769 - val_accuracy: 0.0200 - 230ms/epoch - 15ms/step\n",
            "Epoch 7/37\n",
            "15/15 - 0s - loss: 3.6207 - accuracy: 0.0867 - val_loss: 3.5979 - val_accuracy: 0.0700 - 244ms/epoch - 16ms/step\n",
            "Epoch 8/37\n",
            "15/15 - 0s - loss: 3.4231 - accuracy: 0.0967 - val_loss: 3.3013 - val_accuracy: 0.1600 - 245ms/epoch - 16ms/step\n",
            "Epoch 9/37\n",
            "15/15 - 0s - loss: 3.1442 - accuracy: 0.1767 - val_loss: 3.0480 - val_accuracy: 0.3300 - 250ms/epoch - 17ms/step\n",
            "Epoch 10/37\n",
            "15/15 - 0s - loss: 2.5128 - accuracy: 0.2967 - val_loss: 2.2494 - val_accuracy: 0.4100 - 249ms/epoch - 17ms/step\n",
            "Epoch 11/37\n",
            "15/15 - 0s - loss: 1.9032 - accuracy: 0.4467 - val_loss: 1.5624 - val_accuracy: 0.6100 - 251ms/epoch - 17ms/step\n",
            "Epoch 12/37\n",
            "15/15 - 0s - loss: 1.4987 - accuracy: 0.5967 - val_loss: 1.3099 - val_accuracy: 0.6900 - 260ms/epoch - 17ms/step\n",
            "Epoch 13/37\n",
            "15/15 - 0s - loss: 1.1959 - accuracy: 0.6767 - val_loss: 0.8451 - val_accuracy: 0.7900 - 269ms/epoch - 18ms/step\n",
            "Epoch 14/37\n",
            "15/15 - 0s - loss: 0.9277 - accuracy: 0.7567 - val_loss: 0.5353 - val_accuracy: 0.8800 - 252ms/epoch - 17ms/step\n",
            "Epoch 15/37\n",
            "15/15 - 0s - loss: 0.8936 - accuracy: 0.7500 - val_loss: 0.7567 - val_accuracy: 0.7800 - 243ms/epoch - 16ms/step\n",
            "Epoch 16/37\n",
            "15/15 - 0s - loss: 0.6712 - accuracy: 0.7967 - val_loss: 0.6032 - val_accuracy: 0.8400 - 247ms/epoch - 16ms/step\n",
            "Epoch 17/37\n",
            "15/15 - 0s - loss: 0.5065 - accuracy: 0.8500 - val_loss: 0.6193 - val_accuracy: 0.8400 - 248ms/epoch - 17ms/step\n",
            "Epoch 18/37\n",
            "15/15 - 0s - loss: 0.5198 - accuracy: 0.8400 - val_loss: 0.3221 - val_accuracy: 0.9200 - 243ms/epoch - 16ms/step\n",
            "Epoch 19/37\n",
            "15/15 - 0s - loss: 0.3833 - accuracy: 0.8700 - val_loss: 0.3196 - val_accuracy: 0.8900 - 276ms/epoch - 18ms/step\n",
            "Epoch 20/37\n",
            "15/15 - 0s - loss: 0.3598 - accuracy: 0.8733 - val_loss: 0.3074 - val_accuracy: 0.9100 - 246ms/epoch - 16ms/step\n",
            "Epoch 21/37\n",
            "15/15 - 0s - loss: 0.2481 - accuracy: 0.9167 - val_loss: 0.3103 - val_accuracy: 0.8900 - 237ms/epoch - 16ms/step\n",
            "Epoch 22/37\n",
            "15/15 - 0s - loss: 0.2086 - accuracy: 0.9467 - val_loss: 0.2612 - val_accuracy: 0.9200 - 252ms/epoch - 17ms/step\n",
            "Epoch 23/37\n",
            "15/15 - 0s - loss: 0.2602 - accuracy: 0.9133 - val_loss: 0.4019 - val_accuracy: 0.8800 - 245ms/epoch - 16ms/step\n",
            "Epoch 24/37\n",
            "15/15 - 0s - loss: 0.2669 - accuracy: 0.9200 - val_loss: 0.2321 - val_accuracy: 0.9300 - 258ms/epoch - 17ms/step\n",
            "Epoch 25/37\n",
            "15/15 - 0s - loss: 0.1823 - accuracy: 0.9433 - val_loss: 0.2470 - val_accuracy: 0.9400 - 254ms/epoch - 17ms/step\n",
            "Epoch 26/37\n",
            "15/15 - 0s - loss: 0.1946 - accuracy: 0.9433 - val_loss: 0.3295 - val_accuracy: 0.9200 - 268ms/epoch - 18ms/step\n",
            "Epoch 27/37\n",
            "15/15 - 0s - loss: 0.1707 - accuracy: 0.9400 - val_loss: 0.2055 - val_accuracy: 0.9400 - 244ms/epoch - 16ms/step\n",
            "Epoch 28/37\n",
            "15/15 - 0s - loss: 0.1470 - accuracy: 0.9600 - val_loss: 0.2022 - val_accuracy: 0.9500 - 246ms/epoch - 16ms/step\n",
            "Epoch 29/37\n",
            "15/15 - 0s - loss: 0.1555 - accuracy: 0.9600 - val_loss: 0.1718 - val_accuracy: 0.9500 - 237ms/epoch - 16ms/step\n",
            "Epoch 30/37\n",
            "15/15 - 0s - loss: 0.2078 - accuracy: 0.9367 - val_loss: 0.1837 - val_accuracy: 0.9700 - 257ms/epoch - 17ms/step\n",
            "Epoch 31/37\n",
            "15/15 - 0s - loss: 0.1183 - accuracy: 0.9600 - val_loss: 0.1664 - val_accuracy: 0.9600 - 232ms/epoch - 15ms/step\n",
            "Epoch 32/37\n",
            "15/15 - 0s - loss: 0.1376 - accuracy: 0.9500 - val_loss: 0.1363 - val_accuracy: 0.9500 - 242ms/epoch - 16ms/step\n",
            "Epoch 33/37\n",
            "15/15 - 0s - loss: 0.0922 - accuracy: 0.9700 - val_loss: 0.1060 - val_accuracy: 0.9700 - 236ms/epoch - 16ms/step\n",
            "Epoch 34/37\n",
            "15/15 - 0s - loss: 0.1126 - accuracy: 0.9700 - val_loss: 0.1992 - val_accuracy: 0.9500 - 246ms/epoch - 16ms/step\n",
            "Epoch 35/37\n",
            "15/15 - 0s - loss: 0.1270 - accuracy: 0.9633 - val_loss: 0.1422 - val_accuracy: 0.9700 - 239ms/epoch - 16ms/step\n",
            "Epoch 36/37\n",
            "15/15 - 0s - loss: 0.0619 - accuracy: 0.9767 - val_loss: 0.2859 - val_accuracy: 0.9600 - 250ms/epoch - 17ms/step\n",
            "Epoch 37/37\n",
            "15/15 - 0s - loss: 0.0459 - accuracy: 0.9867 - val_loss: 0.1965 - val_accuracy: 0.9700 - 238ms/epoch - 16ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicting for test data"
      ],
      "metadata": {
        "id": "SS0OKe7Q8UVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "y_test = y_test.reshape(-1,)\n",
        "\n",
        "diff = y_test - y_pred\n",
        "diff = diff.reshape(-1,1)\n",
        "\n",
        "true = 0\n",
        "for i in range(0,len(diff)):\n",
        "    if diff[i] == 0:\n",
        "        true = true + 1\n",
        "\n",
        "Cnn_accuracy = round(100*true/len(diff),2)\n",
        "\n",
        "print(\"Cnn_accuracy is %\", Cnn_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNWuRMvv6b4i",
        "outputId": "74b19b4f-40aa-4862-8092-ef806c66c996"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 4ms/step\n",
            "Cnn_accuracy is % 97.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparing with PCA approach\n",
        "\n",
        "The PCA approach had 83% accuracy whereas CNN has 97% accuracy. CNN was faster and more efficient as compared to the dimensionality reduction based approach"
      ],
      "metadata": {
        "id": "QCxmrUUDGp78"
      }
    }
  ]
}